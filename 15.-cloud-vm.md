# 15. Cloud VM

In a traditional infrastructure, an attacker may find intrusions to be difficult as the network can be isolated from the outside world. In a cloud environment, the attacker may simply need to have an Internet connection and a dictionary of stolen password hashes or SSH keys to cause a breach. A lack of oversight in the security procedures of cloud providers can dramatically increase the risk an organization takes. As a security professional, you must be able to assess the threats and vulnerabilities associated with cloud service and delivery models, plus the virtualization technologies that underpin them.

## CLOUD DEPLOYMENT MODELS

A cloud deployment model classifies how the service is owned and provisioned. It is important to recognize the different impacts deployment models have on threats and vulnerabilities. Cloud deployment models can be broadly categorized as follows:

* Public (or multi-tenant)—a service offered over the Internet by cloud service providers (CSPs) to cloud consumers. With this model, businesses can offer subscriptions or pay-as-you-go financing, while at the same time providing lower-tier services free of charge. As a shared resource, there are risks regarding performance and security. Multi-cloud architectures are where an organization uses services from multiple CSPs.
* Hosted Private—hosted by a third-party for the exclusive use of the organization. This is more secure and can guarantee a better level of performance but is correspondingly more expensive.
* Private—cloud infrastructure that is completely private to and owned by the organization. In this case, there is likely to be one business unit dedicated to managing the cloud while other business units make use of it. With private cloud computing, organizations can exercise greater control over the privacy and security of their services. This type of delivery method is geared more toward banking and governmental services that require strict access control in their operations.

This type of cloud could be on-premise or offsite relative to the other business units. An onsite link can obviously deliver better performance and is less likely to be subject to outages (loss of an Internet link, for instance). On the other hand, a dedicated offsite facility may provide better shared access for multiple users in different locations.

* Community—this is where several organizations share the costs of either a hosted private or fully private cloud. This is usually done in order to pool resources for a common concern, like standardization and security policies.

There will also be cloud computing solutions that implement some sort of hybrid public/private/community/hosted/onsite/offsite solution. For example, a travel organization may run a sales website for most of the year using a private cloud but break out the solution to a public cloud at times when much higher utilization is forecast.

Flexibility is a key advantage of cloud computing, but the implications for data risk must be well understood when moving data between private and public storage environments.

## CLOUD SERVICE MODELS

As well as the ownership model (public, private, hybrid, or community), cloud services are often differentiated on the level of complexity and pre-configuration provided. These models are referred to as something or anything as a service (XaaS). The three most common implementations are infrastructure, software, and platform.

#### Infrastructure as a Service <a href="#ab211f98-ef58-4615-8fe3-d90ad8162dba" id="ab211f98-ef58-4615-8fe3-d90ad8162dba"></a>

Infrastructure as a service (IaaS) is a means of provisioning IT resources such as servers, load balancers, and storage area network (SAN) components quickly. Rather than purchase these components and the Internet links they require, you rent them on an as-needed basis from the service provider's data center. Examples include Amazon Elastic Compute Cloud ([aws.amazon.com/ec2](https://aws.amazon.com/ec2/)), Microsoft Azure Virtual Machines ([azure.microsoft.com/services/virtual-machines](https://azure.microsoft.com/services/virtual-machines)), Oracle Cloud ([oracle.com/cloud](https://www.oracle.com/cloud/)), and OpenStack ([openstack.org](https://www.openstack.org/)).

#### Software as a Service <a href="#id-4f5abe97-33ae-4f3c-8763-8149939bc3f1" id="id-4f5abe97-33ae-4f3c-8763-8149939bc3f1"></a>

Software as a service (SaaS) is a different model of provisioning software applications. Rather than purchasing software licenses for a given number of seats, a business would access software hosted on a supplier's servers on a pay-as-you-go or lease arrangement (on-demand). Virtual infrastructure allows developers to provision on-demand applications much more quickly than previously. The applications can be developed and tested in the cloud without the need to test and deploy on client computers. Examples include Microsoft Office 365 ([microsoft.com/en-us/microsoft-365/enterprise](https://www.microsoft.com/en-us/microsoft-365/enterprise)), Salesforce ([salesforce.com](https://www.salesforce.com/)), and Google G Suite ([gsuite.google.com](https://gsuite.google.com/)).

#### Platform as a Service <a href="#id-2c6d2dc6-bbb3-49fa-bfaf-d7128e09a49b" id="id-2c6d2dc6-bbb3-49fa-bfaf-d7128e09a49b"></a>

Platform as a service (PaaS) provides resources somewhere between SaaS and IaaS. A typical PaaS solution would provide servers and storage network infrastructure (as per IaaS) but also provide a multi-tier web application/database platform on top. This platform could be based on Oracle or MS SQL or PHP and MySQL. Examples include Oracle Database ([oracle.com/database](https://www.oracle.com/database/)), Microsoft Azure SQL Database ([azure.microsoft.com/services/sql-database](https://azure.microsoft.com/services/sql-database)), and Google App Engine ([cloud.google.com/appengine](https://cloud.google.com/appengine)).

As distinct from SaaS though, this platform would not be configured to actually do anything. Your own developers would have to create the software (the CRM or e‑commerce application) that runs using the platform. The service provider would be responsible for the integrity and availability of the platform components, but you would be responsible for the security of the application you created on the platform.

ANYTHING AS A SERVICE

There are many other examples of XaaS, reflecting the idea that anything can be provisioned as a cloud service. For example, database as a service and network as a service can be distinguished as more specific types of platform as a service. The key security consideration with all these models is identifying where responsibilities lie. This is often referred to as security in the cloud versus security of the cloud. Security in the cloud is the things you must take responsibility for; security of the cloud is the things the CSP manages. These responsibilities vary according to the service type:

<table data-header-hidden><thead><tr><th></th><th width="168"></th><th width="159"></th><th></th></tr></thead><tbody><tr><td>Responsibility</td><td>IaaS</td><td>PaaS</td><td>SaaS</td></tr><tr><td>IAM</td><td>You</td><td>You</td><td>You (using CSP toolset)</td></tr><tr><td>Data security (CIA attributes/backup)</td><td>You</td><td>You</td><td>You/CSP/Both</td></tr><tr><td>Data privacy</td><td>You/CSP/Both</td><td>You/CSP/Both</td><td>You/CSP/Both</td></tr><tr><td>Application code/configuration</td><td>You</td><td>You</td><td>CSP</td></tr><tr><td>Virtual network/firewall</td><td>You</td><td>You/CSP</td><td>CSP</td></tr><tr><td>Middleware (database) code/configuration</td><td>You</td><td>CSP</td><td>CSP</td></tr><tr><td>Virtual Guest OS</td><td>You</td><td>CSP</td><td>CSP</td></tr><tr><td>Virtualization layer</td><td>CSP</td><td>CSP</td><td>CSP</td></tr><tr><td>Hardware layer (compute, storage, networking)</td><td>CSP</td><td>CSP</td><td>CSP</td></tr></tbody></table>

## SECURITY AS A SERVICE

The breadth of technologies requiring specialist security knowledge and configuration makes it likely that companies will need to depend on third-party support at some point. You can classify such support in three general "tiers":

* Consultants—the experience and perspective of a third-party professional can be hugely useful in improving security awareness and capabilities in any type of organization (small to large). Consultants could be used for "big picture" framework analysis and alignment or for more specific or product-focused projects (pen testing, SIEM rollout, and so on). It is also fairly simple to control costs when using consultants if they are used to develop capabilities rather than implement them. Where consultants come to "own" the security function, it can be difficult to change or sever the relationship.
* Managed Security Services Provider (MSSP)—a means of fully outsourcing responsibility for information assurance to a third party. This type of solution is expensive but can be a good fit for a SMB that has experienced rapid growth and has no in-house security capability. Of course, this type of outsourcing places a huge amount of trust in the MSSP. Maintaining effective oversight of the MSSP requires a good degree of internal security awareness and expertise. There could also be significant challenges in industries exposed to high degrees of regulation in terms of information processing.
* Security as a Service (SECaaS)—can mean lots of different things, but is typically distinguished from an MSSP as being a means of implementing a particular security control, such as virus scanning or SIEM-like functionality, in the cloud. Typically, there would be a connector to the cloud service installed locally. For example, an antivirus agent would scan files locally but be managed and updated from the cloud provider; similarly a log collector would submit events to the cloud service for aggregation and correlation. Examples include Cloudflare ([cloudflare.com/saas](https://www.cloudflare.com/saas)), Mandiant/FireEye ([fireeye.com/mandiant/managed-detection-and-response.html](https://www.fireeye.com/mandiant/managed-detection-and-response.html)), and SonicWall ([sonicwall.com/solutions/service-provider/security-as-a-service](https://www.sonicwall.com/solutions/service-provider/security-as-a-service/)).

## VIRTUALIZATION TECHNOLOGIES AND HYPERVISOR TYPES

Virtualization means that multiple operating systems can be installed and run simultaneously on a single computer. A virtual platform requires at least three components:

* Host hardware—the platform that will host the virtual environment. Optionally, there may be multiple hosts networked together.
* Hypervisor/Virtual Machine Monitor (VMM)—manages the virtual machine environment and facilitates interaction with the computer hardware and network.
* Guest operating systems, Virtual Machines (VM), or instances—operating systems installed under the virtual environment.

One basic distinction that can be made between virtual platforms is between host and bare metal methods of interacting with the host hardware. In a guest OS (or host-based) system, the hypervisor application (known as a Type II hypervisor) is itself installed onto a host operating system. Examples of host-based hypervisors include VMware Workstation, Oracle Virtual Box, and Parallels Workstation. The hypervisor software must support the host OS.

\
A bare metal virtual platform means that the hypervisor (Type I hypervisor) is installed directly onto the computer and manages access to the host hardware without going through a host OS. Examples include VMware ESXi Server, Microsoft's Hyper-V, and Citrix's XEN Server. The hardware needs only support the base system requirements for the hypervisor plus resources for the type and number of guest OSes that will be installed.

## VIRTUAL DESKTOP INFRASTRUCTURE AND THIN CLIENTS

Virtual desktop infrastructure (VDI) refers to using a VM as a means of provisioning corporate desktops. In a typical VDI, desktop computers are replaced by low-spec, low-power thin client computers. When the thin client starts, it boots a minimal OS, allowing the user to log on to a VM stored on the company server infrastructure. The user makes a connection to the VM using some sort of remote desktop protocol (Microsoft Remote Desktop or Citrix ICA, for instance). The thin client has to find the correct image and use an appropriate authentication mechanism. There may be a 1:1 mapping based on machine name or IP address or the process of finding an image may be handled by a connection broker.

All application processing and data storage in the virtual desktop environment (VDE) or workspace is performed by the server. The thin client computer must only be powerful enough to display the screen image, play audio, and transfer mouse, key commands and video, and audio information over the network. All data is stored on the server, so it is easier to back up and the desktop VMs are easier to support and troubleshoot. They are better "locked" against unsecure user practices because any changes to the VM can easily be overwritten from the template image. With VDI, it is also easier for a company to completely offload their IT infrastructure to a third-party services company.

The main disadvantage is that in the event of a failure in the server and network infrastructure, users have no local processing ability, so downtime events may be more costly in terms of lost productivity.

## APPLICATION VIRTUALIZATION AND CONTAINER VIRTUALIZATION

Application virtualization is a more limited type of VDI. Rather than run the whole client desktop as a virtual platform, the client either accesses an application hosted on a server or streams the application from the server to the client for local processing. Most application virtualization solutions are based on Citrix XenApp (formerly MetaFrame/Presentation Server), though Microsoft has developed an App-V product with its Windows Server range and VMware has the ThinApp product. These solution types are now often used with HTML5 remote desktop apps, referred to as "clientless" because users can access them through ordinary web browser software.

Application cell/container virtualization dispenses with the idea of a hypervisor and instead enforces resource separation at the operating system level. The OS defines isolated "cells" for each user instance to run in. Each cell or container is allocated CPU and memory resources, but the processes all run through the native OS kernel. These containers may run slightly different OS distributions but cannot run guest OSes of different types (you could not run Windows or Ubuntu in a RedHat Linux container, for instance). Alternatively, the containers might run separate application processes, in which case the variables and libraries required by the application process are added to the container.

One of the best-known container virtualization products is Docker ([docker.com](https://www.docker.com/)). Containerization underpins many cloud services. In particular it supports microservices and serverless architecture. Containerization is also being widely used to implement corporate workspaces on mobile devices.

\
VM ESCAPE PROTECTION

VM escaping refers to malware running on a guest OS jumping to another guest or to the host. To do this, the malware must identify that it is running in a virtual environment, which is usually simple to do. One means of doing so is through a timing attack. The classic timing attack is to send multiple usernames to an authentication server and measure the server response times. An invalid username will usually be rejected very quickly, but a valid one will take longer (while the authentication server checks the password). This allows the attacker to harvest valid usernames. Malware can use a timing attack within a guest OS to detect whether it is running in a VM (certain operations may take a distinct amount of time compared to a "real" environment). There are numerous other "signatures" that an attacker could use to detect the presence of virtualized system hardware. The next step in VM escaping is for the attacker to compromise the hypervisor. Security researchers have been focusing on this type of exploit and several vulnerabilities have been found in popular hypervisors.

One serious implication of VM escaping is where virtualization is used for hosted applications. If you have a hosted web server, apart from trusting the hosting provider with your data, you have no idea what other applications might be running in other customers' VMs. For example, consider a scenario where you have an e-commerce web server installed on a virtual server leased from an ISP. If a third-party installs another guest OS with malware that can subvert the virtual server's hypervisor, they might be able to gain access to your server or to data held in the memory of the physical server. Having compromised the hypervisor, they could make a copy of your server image and download it to any location. This would allow the attacker to steal any unencrypted data held on the e-commerce server. Even worse, it could conceivably allow them to steal encrypted data, by obtaining the private encryption keys stored on the server or by sniffing unencrypted data or a data encryption key from the physical server's memory.

It is imperative to monitor security bulletins for the hypervisor software that you operate and to install patches and updates promptly. You should also design the VM architecture carefully so that the placement of VMs running different types of applications with different security requirements does not raise unnecessary risks.

Preventing VM escaping is dependent on the virtualization vendor identifying security vulnerabilities in the hypervisor and on these being patched. The impact of VM escaping can be reduced by using effective service design and network placement when deploying VMs.

For example, when considering security zones such as a DMZ, VMs providing front-end and middleware/back-end services should be separated to different physical hosts. This reduces the security implications of a VM escaping attack on a host in the DMZ (which will generally be more vulnerable to such attacks).

VM SPRAWL AVOIDANCE

As well as securing the hypervisor, you must also treat each VM as you would any other network host. This means using security policies and controls to ensure the confidentiality, integrity, and availability of all data and services relying on host virtualization.

Each VM needs to be installed with its own security software suite to protect against malware and intrusion attempts. Each guest must also have a patch management process. This might mean installing updates locally or replacing the guest instance from an updated VM template image.

Although one of the primary benefits of virtualization is the ease of deploying new systems, this type of system sprawl and deployment of undocumented assets can also be the root of security issues. It will often be the case that a system will be brought up for "just a minute" to test something, but languish for months or years, undocumented, unsecured, and unpatched. Each of these undocumented systems could represent an exploitable vulnerability. They increase the potential attack surface of the network. Policies and procedures for tracking, securing, and, when no longer used, destroying virtualized assets should be put in place and carefully enforced.

Virtual machine life cycle management (VMLM) software can be deployed to enforce VM sprawl avoidance. VMLM solutions provide you with a centralized dashboard for maintaining and monitoring all the virtual environments in your organization. More generally, the management procedures for developing and deploying machine images need to be tightly drafted and monitored. VMs should conform to an application-specific template with the minimum configuration needed to run that application (that is, not running unnecessary services). Images should not be run in any sort of environment where they could be infected by malware or have any sort of malicious code inserted. One of the biggest concerns here is of rogue developers or contractors installing backdoors or "logic bombs" within a machine image. The problem of criminal or disgruntled staff is obviously one that affects any sort of security environment, but concealing code within VM machine images is a bit easier to accomplish and has the potential to be much more destructive.\\

## Apply Cloud Security Solutions

CLOUD SECURITY INTEGRATION AND AUDITING

Cloud-based services must be integrated within regular security policies and procedures and audited for compliance. Where indicators of on-premises attacks are found in local application logs and network traffic, indicators of cloud-based attacks are found in API logs and metrics. The same correlation to suspicious IP address ranges and domains and suspicious code strings must be made, but the source of this data is the cloud service provider (CSP). Accessing this auditing information in real time may be difficult, depending on the cloud service type. There are many cloud-based SIEM solutions that can perform this collection, aggregation, and correlation of security data from both on-premises and cloud-based networks and instances.

As with any contracted service, cloud computing is a means of transferring risk. As such, it is imperative to identify precisely which risks you are transferring, to identify which responsibilities the service provider is undertaking, and to identify which responsibilities remain with you. This should be set out in a service level agreement (SLA) with a responsibility matrix. For example, in an SaaS solution, the provider may be responsible for the confidentiality, integrity, and availability of the software. They would be responsible for configuring a fault tolerant, clustered server service; for firewalling the servers and creating proper authentication, authorization, and accounting procedures; for scanning for intrusions and monitoring network logs, applying OS and software patches; and so on. You might or might not be responsible for some or all of the software management functions, though—ensuring that administrators and users practice good password management, configuring system privileges, making backups of data, and so on.

Where critical tasks are the responsibility of the service provider, you should try to ensure that there is a reporting mechanism to show that these tasks are being completed, that their disaster recovery plans are effective, and so on.

Another proviso is that your company is likely to still be directly liable for serious security breaches; if customer data is stolen, for instance, or if your hosted website is hacked and used to distribute malware. You still have liability for legal and regulatory requirements. You might be able to sue the service provider for damages, but your company would still be the point of investigation. You may also need to consider the legal implications of using a cloud provider if its servers are located in a different country.

You must also consider the risk of insider threat, where the insiders are administrators working for the service provider. Without effective security mechanisms such as separation of duties and quorum authentication (also known as M of N access control), it is highly likely that they would be able to gain privileged access to your data. Consequently, the service provider must be able to demonstrate to your satisfaction that they are prevented from doing so. There is also the risk described earlier that your data is in proximity to other, unknown virtual servers and that some sort of attack could be launched on your data from another virtual server.

As with any contracted service, with any \*aaS solution, you place a large amount of trust in the service provider. The more important the service is to your business, the more risk you are investing in that trust relationship.

CLOUD SECURITY CONTROLS

Clouds use the same types of security controls as on-premises networks, including identity and access management (IAM), endpoint protection (for virtual instances), resource policies to govern access to data and services, firewalls to filter traffic between hosts, and logging to provide an audit function.

Most CSP's will provide these security controls as native functionality of the cloud platform. Google's firewall service is an example of this type of cloud-native control ([cloud.google.com/firewalls](https://cloud.google.com/firewalls)). The controls can be deployed and configured using either the CSP's web console, or programmatically via a command line interface (CLI) or application programming interface (API). A third-party solution would typically be installed as a virtual instance within the cloud. For example, you might prefer to run a third-party next-generation firewall. This can be configured as an appliance and deployed to the cloud. The virtual network architecture can be defined so that this appliance instance is able to inspect traffic and apply policies to it, either by routing the traffic through the instance or by using some type of bridging or mirroring. As an example, consider the configuration guide for the Barracuda next-gen firewall ([campus.barracuda.com/product/cloudgenfirewall/doc/79462645/overview](https://campus.barracuda.com/product/cloudgenfirewall/doc/79462645/overview)).

The same considerations can be made for other types of security controls—notably data loss prevention and compliance management. Cloud-native controls might not exist for these use cases, they might not meet the functional requirements that third-party solutions can, and there may be too steep a transition in terms of change management and skills development.

#### Application Security and IAM <a href="#c22e7485-ba99-4480-bf75-bfb9c563f27b" id="c22e7485-ba99-4480-bf75-bfb9c563f27b"></a>

Application security in the cloud refers both to the software development process and to identity and access management (IAM) features designed to ensure authorized use of applications.

Just as with on-premises solutions, cloud-based IAM enables the creation of user and user security groups, plus role-based management of privileges.

#### Secrets Management <a href="#fdffb537-9b5d-4cdb-add8-e135383cdde2" id="fdffb537-9b5d-4cdb-add8-e135383cdde2"></a>

A cloud service is highly vulnerable to remote access. A failure of credential management is likely to be exploited by malicious actors. You must enforce strong authentication policies to mitigate risks:

* Do not use the root user for the CSP account for any day-to-day logon activity.
* Require strong multifactor authentication (MFA) for interactive logons. Use conditional authentication to deny or warn of risky account activity.
* Principals—user accounts, security groups, roles, and services—can interact with cloud services via CLIs and APIs. Such programmatic access is enabled by assigning a secret key to the account. Only the secret key (not the ordinary account credential) can be used for programmatic access. When a secret key is generated for an account, it must immediately be transferred to the host and kept securely on that host.

## CLOUD COMPUTE SECURITY

Cloud provides resources abstracted from physical hardware via one or more layers of virtualization. The compute component provides process and system memory (RAM) resource as required for a particular workload. The workload could be a virtual machine instance configured with four CPUs and 16 GB RAM or it could be a container instance spun up to perform a function and return a result within a given timeframe. The virtualization layer ensures that the resources required for this task are made available on-demand. This can be referred to as dynamic resource allocation. It will be the responsibility of the CSP to ensure this capability is met to the standards agreed in the SLA.

Within the compute component, the following critical security considerations can be identified.

#### Container Security <a href="#f026412d-59f0-4b0e-8481-43b4af9593a2" id="f026412d-59f0-4b0e-8481-43b4af9593a2"></a>

A container uses many shared components on the underlying platform, meaning it must be carefully configured to reduce the risk of data exposure. In a container engine such as Docker, each container is isolated from others through separate namespaces and control groups ([docs.docker.com/engine/security/security](https://docs.docker.com/engine/security/security/)). Namespaces prevent one container reading or writing processes in another, while control groups ensure that one container cannot overwhelm others in a DoS-type attack.

#### API Inspection and Integration <a href="#id-3d99492c-335e-480a-8ec4-a27a87aa6943" id="id-3d99492c-335e-480a-8ec4-a27a87aa6943"></a>

The API is the means by which consumers interact with the cloud infrastructure, platform, or application. The consumer may use direct API calls, or may use a CSP-supplied web console as a graphical interface for the API. Monitoring API usage gives warning if the system is becoming overloaded (ensuring availability) and allows detection of unauthorized usage or attempted usage.

* Number of requests—this basic load metric counts number of requests per second or requests per minute. Depending on the service type, you might be able to establish baselines for typical usage and set thresholds for alerting abnormal usage. An unexplained spike in API calls could be an indicator of a DDoS attack, for instance.
* Latency—this is the time in milliseconds (ms) taken for the service to respond to an API call. This can be measured for specific services or as an aggregate value across all services. High latency usually means that compute resources are insufficient. The cause of this could be genuine load or DDoS, however.
* Error rates—this measures the number of errors as a percentage of total calls, usually classifying error types under category headings. Errors may represent an overloaded system if the API is unresponsive, or a security issue, if the errors are authorization/access denied types.
* Unauthorized and suspicious endpoints—connections to the API can be managed in the same sort of way as remote access. The client endpoint initiating the connection can be restricted using an ACL and the endpoint's IP address monitored for geographic location.

#### Instance Awareness <a href="#c4a37bf8-96ba-4585-b1f3-3fa5b2e61ece" id="c4a37bf8-96ba-4585-b1f3-3fa5b2e61ece"></a>

As with on-premises virtualization, it is important to manage instances (virtual machines and containers) to avoid sprawl, where undocumented instances are launched and left unmanaged. As well as restricting rights to launch instances, you should configure logging and monitoring to track usage.

CLOUD STORAGE SECURITY

Where the compute component refers to CPU and system memory resources, the storage component means the provisioning of persistent storage capacity. As with the compute component, the cloud virtualization layer abstracts the underlying hardware to provide the required storage type, such as a virtual hard disk for a VM instance, object-based storage to serve static files in a web application, or block storage for use by a database server. Storage profiles will have different performance characteristics for different applications, such as fast SSD-backed storage for databases versus slower HDD-backed media for archiving. The principal performance metric is the number of input/output operations per second (IOPS) supported.

#### Permissions and Resource Policies <a href="#id-61888908-13d0-4b86-a444-e6c7ee38c371" id="id-61888908-13d0-4b86-a444-e6c7ee38c371"></a>

As with on-premises systems, cloud storage resources must be configured to allow reads and/or writes only from authorized endpoints. In the cloud, a resource policy acts as the ACL for an object. In a resource policy, permissions statements are typically written as a JavaScript Object Notation (JSON) strings. Misconfiguration of these resource policies is a widely exploited attack vector. For example, the following policy uses the "any" wildcard (\*) to assign both actions (read and write) and principals (accounts) to a storage object. This type of policy breaks the principle of least privilege and is highly unsecure:

"Statement": \[ {

"Action": \[

"\*"

],

"Effect": "Allow",

"Principal": "\*",

"Resource": "arn:aws:s3:::515support-courses-data/\*"

} ]

#### Encryption <a href="#id-5bad4020-ec24-4875-9561-6a260938767e" id="id-5bad4020-ec24-4875-9561-6a260938767e"></a>

Cloud storage encryption equates to the on-premises concept of full disk encryption (FDE). The purpose is to minimize the risk of data loss via an insider or intruder attack on the CSP's storage systems. Each storage unit is encrypted using an AES key. If an attacker were to physically access a data center and copy or remove a disk, the data on the disk would not be readable.

To read or write the data, the AES key must be available to the VM or container using the storage object. With CSP-managed keys, the cloud provider handles this process by using the access control rights configured on the storage resource to determine whether access is approved and, if so, making the key available to the VM or container. The key will be stored in a hardware security module (HSM) within the cloud. The HSM and separation of duties policies protect the keys from insider threat. Alternatively, customers can manage keys themselves, taking on all responsibility for secure distribution and storage.

Encryption can also be applied at other levels. For example, applications can selectively encrypt file system objects or use database-level encryption to encrypt fields and/or records. All networking—whether customer to cloud or between VMs/containers within the cloud—should use encrypted protocols such as HTTPS or IPSec.

HIGH AVAILABILITY

One of the benefits of the cloud is the potential for providing services that are resilient to failures at different levels, such as component, server, local network, site, data center, and wide area network. The CSP uses a virtualization layer to ensure that compute, storage, and network provision meet the availability criteria set out in its SLA. In terms of storage performance tiers, high availability (HA) refers to storage provisioned with a guarantee of 99.99% uptime or better. As with on-premises architecture, the CSP uses redundancy to make multiple disk controllers and storage devices available to a pool of storage resource. Data may be replicated between pools or groups, with each pool supported by separate hardware resources.

#### Replication <a href="#id-6cca491e-9745-48cb-be01-5139345646bf" id="id-6cca491e-9745-48cb-be01-5139345646bf"></a>

Data replication allows businesses to copy data to where it can be utilized most effectively. The cloud may be used as a central storage area, making data available among all business units. Data replication requires low-latency network connections, security, and data integrity. CSPs offer several data storage performance tiers ([cloud.google.com/storage/docs/storage-classes](https://cloud.google.com/storage/docs/storage-classes)). The terms hot and cold storage refer to how quickly data is retrieved. Hot storage retrieves data more quickly than cold, but the quicker the data retrieval, the higher the cost. Different applications have diverse replication requirements. A database generally needs low-latency, synchronous replication, as a transaction often cannot be considered complete until it has been made on all replicas. A mechanism to replicate data files to backup storage might not have such high requirements, depending on the criticality of the data.

#### High Availability across Zones <a href="#id-6a9163e8-8802-441d-9c9c-7d75f733de3a" id="id-6a9163e8-8802-441d-9c9c-7d75f733de3a"></a>

CSPs divide the world into regions. Each region is independent of the others. The regions are divided into availability zones. The availability zones have independent data centers with their own power, cooling, and network connectivity. You can choose to host data, services, and VM instances in a particular region to provide a lower latency service to customers. Provisioning resources in multiple zones and regions can also improve performance and increases redundancy, but requires an adequate level of replication performance.

Consequently, CSPs offer several tiers of replication representing different high availability service levels:

* Local replication—replicates your data within a single data center in the region where you created your storage account. The replicas are often in separate fault domains and upgrade domains.
* Regional replication (also called zone-redundant storage)—replicates your data across multiple data centers within one or two regions. This safeguards data and access in the event a single data center is destroyed or goes offline.
* Geo-redundant storage (GRS)—replicates your data to a secondary region that is distant from the primary region. This safeguards data in the event of a regional outage or a disaster.

## CLOUD NETWORKING SECURITY

Within the cloud, the Cloud Service Provider (CSP) establishes a virtualization layer that abstracts the underlying physical network. This allows the CSP to operate a public cloud where the networking performed by each customer account is isolated from the others. In terms of customer-configured cloud networking, there are various contexts:

* Networks by which the cloud consumer operates and manages the cloud systems.
* Virtual networks established between VMs and containers within the cloud.
* Virtual networks by which cloud services are published to guests or customers on the Internet.

#### Virtual Private Clouds (VPCs A private network segment made available to a single cloud consumer on a public cloud.) <a href="#id-22d9acf4-912d-48b1-836f-b9d257981419" id="id-22d9acf4-912d-48b1-836f-b9d257981419"></a>

Each customer can create one or more virtual private clouds (VPCs) attached to their account. By default, a VPC is isolated from other CSP accounts and from other VPCs operating in the same account. This means that customer A cannot view traffic passing over customer B's VPC. The workload for each VPC is isolated from other VPCs. Within the VPC, the cloud consumer can assign an IPv4 CIDR block and configure one or more subnets within that block. Optionally, an IPv6 CIDR block can be assigned also.

#### Public and Private Subnets <a href="#id-5e39fa53-aef6-4cfb-849f-38430349eb50" id="id-5e39fa53-aef6-4cfb-849f-38430349eb50"></a>

Each subnet within a VPC can either be private or public. To configure a public subnet, first an Internet gateway (virtual router) must be attached to the VPC configuration. Secondly, the Internet gateway must be configured as the default route for each public subnet. If a default route is not configured, the subnet remains private, even if an Internet gateway is attached to the VPC. Each instance in the subnet must also be configured with a public IP in its cloud profile. The Internet gateway performs 1:1 network address translation (NAT) to route Internet communications to and from the instance.

There are other ways to provision external connectivity for a subnet if it is not appropriate to make it public:

* NAT gateway—this feature allows an instance to connect out to the Internet or to other services in AWS, but does not allow connections initiated from the Internet.
* VPN—there are various options for establishing connections to and between VPCs using virtual private networks (VPNs) at the software layer or using CSP-managed features.

## VPCS AND TRANSIT GATEWAYS

Routing can be configured between subnets within a VPC. This traffic can be subject to cloud native ACLs allowing or blocking traffic on the basis of host IPs and ports. Alternatively, traffic could be routed through a virtual firewall instance, or other security appliance.

Connectivity can also be configured between VPCs in the same account or with VPCs belonging to different accounts, and between VPCs and on-premises networks. Configuring additional VPCs rather than subnets within a VPC allows for a greater degree of segmentation between instances. A complex network might split segments between different VPCs across different cloud accounts for performance or compliance reasons.

Traditionally, VPCs can be interconnected using peering relationships and connected with on-premises networks using VPN gateways. These one-to-one VPC peering relationships can quickly become difficult to manage, especially if each VPC must interconnect in a mesh-like structure. In cloud computing, a virtual router deployed to facilitate connections between VPC subnets and VPN gateways. A transit gateway is a simpler means of managing these interconnections. Essentially, a transit gateway is a virtual router that handles routing between the subnets in each attached VPC and any attached VPN gateways ([aws.amazon.com/transit-gateway](https://aws.amazon.com/transit-gateway/)).

VPC ENDPOINTS

A VPC endpoint is a means of publishing a service so that it is accessible by instances in other VPCs using only the AWS internal network and private IP addresses ([d1.awsstatic.com/whitepapers/aws-privatelink.pdf](http://d1.awsstatic.com/whitepapers/aws-privatelink.pdf)). This means that the traffic is never exposed to the Internet. There are two types of VPC endpoint: gateway and interface.

#### &#x20;<a href="#e5bdf384-6722-482e-b47f-504be0afdfc7" id="e5bdf384-6722-482e-b47f-504be0afdfc7"></a>

Gateway Endpoints

A gateway endpoint is used to connect instances in a VPC to the AWS S3 (storage) and DynamoDB (database) services. A gateway endpoint is configured as a route to the service in the VPC's route table.

Interface Endpoints

An interface endpoint makes use of AWS's PrivateLink feature to allow private access to custom services:

* A custom service provider VPC is configured by publishing the service with a DNS host name. Alternatively, the service provider might be an Amazon default service that is enabled as a VPC interface endpoint, such as CloudWatch Events/Logs.
* A VPC endpoint interface is configured in each service consumer VPC subnet. The VPC endpoint interface is configured with a private IP address within the subnet plus the DNS host name of the service provider.
* Each instance within the VPC subnet is configured to use the endpoint address to contact the service provider.

## CLOUD FIREWALL SECURITY

As in an on-premises network, a firewall determines whether to accept or deny/discard incoming and outgoing traffic. Firewalls work with multiple accounts, VPCs, subnets within VPCs, and instances within subnets to enforce the segmentation required by the architectural design. Segmentation may be needed for many different reasons, including separating workloads for performance and load balancing, keeping data processing within an isolated segment for compliance with laws and regulations, and compartmentalizing data access and processing for different departments or functional requirements.

Filtering decisions can be made based on packet headers and payload contents at various layers, identified in terms of the OSI model:

* Network layer (layer 3)—the firewall accepts or denies connections on the basis of IP addresses or address ranges and TCP/UDP port numbers (the latter are actually contained in layer 4 headers, but this functionality is still always described as basic layer 3 packet filtering).
* Transport layer (layer 4)—the firewall can store connection states and use rules to allow established or related traffic. Because the firewall must maintain a state table of existing connections, this requires more processing power (CPU and memory).
* Application layer (layer 7)—the firewall can parse application protocol headers and payloads (such as HTTP packets) and make filtering decisions based on their contents. This requires even greater processing capacity (or load balancing), or the firewall will become a bottleneck and increase network latency.

While you can use cloud-based firewalls to implement on-premises network security, here we are primarily concerned with the use of firewalls to filter traffic within and to and from the cloud itself. Such firewalls can be implemented in several ways to suit different purposes:

* As software running on an instance. This sort of host-based firewall is identical to ones that you would configure for an on-premises host. It could be a stateful packet filtering firewall or a web application firewall (WAF) with a ruleset tuned to preventing malicious attacks. The drawback is that the software consumes instance resources and so is not very efficient. Also, managing the rulesets across many instances can be challenging.
* As a service at the virtualization layer to filter traffic between VPC subnets and instances. This equates to the concept of an on-premises network firewall.

Native cloud application-aware firewalls incur transaction costs, typically calculated on time deployed and traffic volume. These costs might be a reason to choose a third-party solution instead of the native control.

SECURITY GROUPS

In AWS, basic packet filtering rules managing traffic that each instance will accept can be managed through security groups ([docs.aws.amazon.com/vpc/latest/userguide/VPC\_SecurityGroups.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)). A security group provides stateful inbound and outbound filtering at layer 4. The stateful filtering property means that it will allow established and related traffic if a new connection has been accepted.

The default security group allows any outbound traffic and any inbound traffic from instances also bound to the default security group. A custom security group sets the ports and endpoints that are allowed for inbound and outbound traffic. There are no deny rules for security groups; any traffic that does not match an allow rule is dropped. Consequently, a custom group with no rules will drop all network traffic. Multiple instances can be assigned to the same security group, and instances within the same subnet can be assigned to different security groups. You can assign multiple security groups to the same instance. You can also assign security groups to VPC endpoint interfaces.

Most cloud providers support similar filtering functionality, though they may be implemented differently. For example, in Azure, network security groups can be applied to network interfaces or to subnets ([docs.microsoft.com/en-us/azure/virtual-network/security-overview](https://docs.microsoft.com/en-us/azure/virtual-network/security-overview)).<br>

## CLOUD ACCESS SECURITY BROKERS

A cloud access security broker (CASB) is enterprise management software designed to mediate access to cloud services by users across all types of devices. CASB vendors include Blue Coat, now owned by Symantec ([broadcom.com/products/cyber-security/information-protection/cloud-application-security-cloudsoc](https://www.broadcom.com/products/cyber-security/information-protection/cloud-application-security-cloudsoc)), SkyHigh Networks, now owned by McAfee ([skyhighnetworks.com](https://www.skyhighnetworks.com/)), Forcepoint ([forcepoint.com/product/casb-cloud-access-security-broker](https://www.forcepoint.com/product/casb-cloud-access-security-broker)), Microsoft Cloud App Security ([microsoft.com/en-us/microsoft-365/enterprise-mobility-security/cloud-app-security](https://www.microsoft.com/en-us/microsoft-365/enterprise-mobility-security/cloud-app-security)), and Cisco Cloudlock ([cisco.com/c/en/us/products/security/cloudlock/index.html](https://www.cisco.com/c/en/us/products/security/cloudlock/index.html)).

CASBs provide you with visibility into how clients and other network nodes are using cloud services. Some of the functions of a CASB are:

* Enable single sign-on authentication and enforce access controls and authorizations from the enterprise network to the cloud provider.
* Scan for malware and rogue or non-compliant device access.
* Monitor and audit user and resource activity.
* Mitigate data exfiltration by preventing access to unauthorized cloud services from managed devices.

In general, CASBs are implemented in one of three ways:

* Forward proxy—this is a security appliance or host positioned at the client network edge that forwards user traffic to the cloud network if the contents of that traffic comply with policy. This requires configuration of users' devices or installation of an agent. In this mode, the proxy can inspect all traffic in real time, even if that traffic is not bound for sanctioned cloud applications. The problem with this mode is that users may be able to evade the proxy and connect directly. Proxies are also associated with poor performance as without a load balancing solution, they become a bottleneck and potentially a single point of failure.
* Reverse proxy—this is positioned at the cloud network edge and directs traffic to cloud services if the contents of that traffic comply with policy. This does not require configuration of the users' devices. This approach is only possible if the cloud application has proxy support.
* Application programming interface (API)—rather than placing a CASB appliance or host inline with cloud consumers and the cloud services, an API-based CASB brokers connections between the cloud service and the cloud consumer. For example, if a user account has been disabled or an authorization has been revoked on the local network, the CASB would communicate this to the cloud service and use its API to disable access there too. This depends on the API supporting the range of functions that the CASB and access and authorization policies demand. CASB solutions are quite likely to use both proxy and API modes for different security management purposes.

#### Next-Generation Secure Web Gateway <a href="#id-602c750c-815c-4cf4-822b-ad96aa2884a5" id="id-602c750c-815c-4cf4-822b-ad96aa2884a5"></a>

Enterprise networks often make use of secure web gateways (SWG). An on-premises SWG is a proxy-based firewall, content filter, and intrusion detection/prevention system that mediates user access to Internet sites and services. A next-generation SWG, as marketed by Netskope ([netskope.com/products/next-gen-swg](https://www.netskope.com/products/next-gen-swg)), combines the functionality of an SWG with that of data loss prevention (DLP) and a CASB to provide a wholly cloud-hosted platform for client access to websites and cloud apps. This supports an architecture defined by Gartner as secure access service edge (SASE) ([https://www.paloaltonetworks.com/cyberpedia/what-is-sase](https://www.paloaltonetworks.com/cyberpedia/what-is-sase)).

## SERVICES INTEGRATION AND MICROSERVICES

In the early days of computer networks, architecture was focused on the provision of server machines and intermediate network systems (switches and routers). Architectural choices centered around where to place a "box" to run monolithic network applications such as routing, security, address allocation, name resolution, file sharing, email, and so on. With virtualization, the provision of these applications is much less dependent on where you put the box and the OS that the box runs. Virtualization helps to make the design architecture fit to the business requirement rather than accommodate the business workflow to the platform requirement.

#### Service-Oriented Architecture (SOA) <a href="#cdfce05b-fc0f-4363-8e9e-7c12509f0cb1" id="cdfce05b-fc0f-4363-8e9e-7c12509f0cb1"></a>

Service-oriented architecture (SOA) conceives of atomic services closely mapped to business workflows. Each service takes defined inputs and produces defined outputs. The service may itself be composed of sub-services. The key features of a service function are that it is self-contained, does not rely on the state of other services, and exposes clear input/output (I/O) interfaces. Because each service has a simple interface, interoperability is made much easier than with a complex monolithic application. The implementation of a service does not constrain compatibility choices for client services, which can use a different platform or development language. This independence of the service and the client requesting the service is referred to as loose coupling.

#### Microservices <a href="#f53bedfc-8440-46e7-9b41-96339ff5a0c2" id="f53bedfc-8440-46e7-9b41-96339ff5a0c2"></a>

Microservice-based development shares many similarities with Agile software project management and the processes of continuous delivery and deployment. It also shares roots with the Unix philosophy that each program or tool should do one thing well. The main difference between SOA and microservices is that SOA allows a service to be built from other services. By contrast, each microservice should be capable of being developed, tested, and deployed independently. The microservices are said to be highly decoupled rather than just loosely decoupled.

Services Integration and Orchestration

Services integration refers to ways of making these decoupled service or microservice components work together to perform a workflow. Where SOA used the concept of an enterprise service bus, microservices integration and cloud services/virtualization/automation integration generally is very often implemented using orchestration tools. Where automation focuses on making a single, discrete task easily repeatable, orchestration performs a sequence of automated tasks. For example, you might orchestrate adding a new VM to a load-balanced cluster. This end-to-end process might include provisioning the VM, configuring it, adding the new VM to the load-balanced cluster, and reconfiguring the load-balancing weight distribution given the new cluster configuration. In doing this, the orchestrated steps would have to run numerous automated scripts or API service calls.

For orchestration to work properly, automated steps must occur in the right sequence, taking dependencies into account; it must provide the right security credentials at every step along the way; and it must have the rights and permissions to perform the defined tasks. Orchestration can automate processes that are complex, requiring dozens or hundreds of manual steps.

Cloud orchestration platforms connect to and provide administration, management, and orchestration for many popular cloud platforms and services. One of the advantages of using a third-party orchestration platform is protection from vendor lock in. If you wish to migrate from one cloud provider to another, or wish to move to a multi-cloud environment, automated workflows can often be adapted for use on new platforms. Industry leaders in this space include Chef ([chef.io](https://www.chef.io/)), Puppet ([puppet.com](https://puppet.com/)), Ansible ([ansible.com](https://www.ansible.com/)), and Kubernetes ([kubernetes.io](https://kubernetes.io/)).

* Simple Object Access Protocol (SOAP)—uses XML format messaging and has a number of extensions in the form of Web Services (WS) standards that support common features, such as authentication, transport security, and asynchronous messaging. SOAP also has a built-in error handling.
* Representational State Transfer (REST)—where SOAP is a tightly specified protocol, REST is a looser architectural framework, also referred to as RESTful APIs. Where a SOAP request must be sent as a correctly formatted XML document, a REST request can be submitted as an HTTP operation/verb (GET or POST for example). Each resource or endpoint in the API, expressed as a noun, should be accessed via a single URL.

Whether based on SOA or microservices, service integration, automation, and orchestration all depend on application programming interfaces (APIs). The service API is the means by which external entities interact with the service, calling it with expected parameters and receiving the expected output. There are two predominant "styles" for creating web application APIs:

## APPLICATION PROGRAMMING INTERFACES

Whether based on SOA or microservices, service integration, automation, and orchestration all depend on application programming interfaces (APIs). The service API is the means by which external entities interact with the service, calling it with expected parameters and receiving the expected output. There are two predominant "styles" for creating web application APIs:

* Simple Object Access Protocol (SOAP)—uses XML format messaging and has a number of extensions in the form of Web Services (WS) standards that support common features, such as authentication, transport security, and asynchronous messaging. SOAP also has a built-in error handling.
* Representational State Transfer (REST)—where SOAP is a tightly specified protocol, REST is a looser architectural framework, also referred to as RESTful APIs. Where a SOAP request must be sent as a correctly formatted XML document, a REST request can be submitted as an HTTP operation/verb (GET or POST for example). Each resource or endpoint in the API, expressed as a noun, should be accessed via a single URL.

SERVERLESS ARCHITECTURE

Serverless is a modern design pattern for service delivery. It is strongly associated with modern web applications—most notably Netflix ([aws.amazon.com/solutions/case-studies/netflix-and-aws-lambda](https://aws.amazon.com/solutions/case-studies/netflix-and-aws-lambda/))—but providers are appearing with products to completely replace the concept of the corporate LAN. With serverless, all the architecture is hosted within a cloud, but unlike "traditional" virtual private cloud (VPC) offerings, services such as authentication, web applications, and communications aren't developed and managed as applications running on VM instances located within the cloud. Instead, the applications are developed as functions and microservices, each interacting with other functions to facilitate client requests. When the client requires some operation to be processed, the cloud spins up a container to run the code, performs the processing, and then destroys the container. Billing is based on execution time, rather than hourly charges. This type of service provision is also called function as a service (FaaS). FaaS products include AWS Lambda ([aws.amazon.com/lambda](https://aws.amazon.com/lambda/)), Google Cloud Functions ([cloud.google.com/functions](https://cloud.google.com/functions)), and Microsoft Azure Functions ([azure.microsoft.com/services/functions](https://azure.microsoft.com/en-us/services/functions/)).

The serverless paradigm eliminates the need to manage physical or virtual server instances, so there is no management effort for software and patches, administration privileges, or file system security monitoring. There is no requirement to provision multiple servers for redundancy or load balancing. As all of the processing is taking place within the cloud, there is little emphasis on the provision of a corporate network. This underlying architecture is managed by the service provider. The principal network security job is to ensure that the clients accessing the services have not been compromised in a way that allows a malicious actor to impersonate a legitimate user. This is a particularly important consideration for the developer accounts and devices used to update the application code underpinning the services. These workstations must be fully locked down, running no other applications or web code than those necessary for development.

Serverless does have considerable risks. As a new paradigm, use cases and best practices are not mature, especially as regards security. There is also a critical and unavoidable dependency on the service provider, with limited options for disaster recovery should that service provision fail.

Serverless architecture depends heavily on the concept of event-driven orchestration to facilitate operations. For example, when a client connects to an application, multiple services will be called to authenticate the user and device, identify the device location and address properties, create a session, load authorizations for the action, use application logic to process the action, read or commit information from a database, and write a log of the transaction. This design logic is different from applications written to run in a "monolithic" server-based environment. This means that adapting existing corporate software will require substantial development effort.

INFRASTRUCTURE AS CODE

The use of cloud technologies encourages the use of scripted approaches to provisioning, rather than manually making configuration changes, or installing patches. An approach to infrastructure management where automation and orchestration fully replace manual configuration is referred to as infrastructure as code (IaC).

One of the goals of IaC is to eliminate snowflake systems. A snowflake is a configuration or build that is different from any other. The lack of consistency—or drift—in the platform environment leads to security issues, such as patches that have not been installed, and stability issues, such as scripts that fail to run because of some small configuration difference. By rejecting manual configuration of any kind, IaC ensures idempotence. Idempotence means that making the same call with the same parameters will always produce the same result. Note that IaC is not simply a matter of using scripts to create instances. Running scripts that have been written ad hoc is just as likely to cause environment drift as manual configuration. IaC means using carefully developed and tested scripts and orchestration runbooks to generate consistent builds.

SOFTWARE-DEFINED NETWORKING

IaC is partly facilitated by physical and virtual network appliances that are fully configurable via scripting and APIs. As networks become more complex—perhaps involving thousands of physical and virtual computers and appliances—it becomes more difficult to implement network policies, such as ensuring security and managing traffic flow. With so many devices to configure, it is better to take a step back and consider an abstracted model about how the network functions. In this model, network functions can be divided into three "planes":

* Control plane—makes decisions about how traffic should be prioritized and secured, and where it should be switched.
* Data plane—handles the actual switching and routing of traffic and imposition of security access controls.
* Management plane—monitors traffic conditions and network status.

A software-defined networking (SDN) application can be used to define policy decisions on the control plane. These decisions are then implemented on the data plane by a network controller application, which interfaces with the network devices using APIs. The interface between the SDN applications and the SDN controller is described as the "northbound" API, while that between the controller and appliances is the "southbound" API. SDN can be used to manage compatible physical appliances, but also virtual switches, routers, and firewalls. The architecture supporting rapid deployment of virtual networking using general-purpose VMs and containers is called network functions virtualization (NFV) ([redhat.com/en/topics/virtualization/what-is-nfv](https://www.redhat.com/en/topics/virtualization/what-is-nfv)).

This architecture saves network and security administrators the job and complexity of configuring each appliance with proper settings to enforce the desired policy. It also allows for fully automated deployment (or provisioning) of network links, appliances, and servers. This makes SDN an important part of the latest automation and orchestration technologies.

## SOFTWARE-DEFINED VISIBILITY

Where SDN addresses secure network "build" solutions, software-defined visibility (SDV) supports assessment and incident response functions. Visibility is the near real-time collection, aggregation, and reporting of data about network traffic flows and the configuration and status of all the hosts, applications, and user accounts participating in it.

SDV can help the security data collection process by gathering statistics from the forwarding systems and then applying a classification scheme to those systems to detect network traffic that deviates from baseline levels ([gigamon.com/content/dam/resource-library/english/white-paper/wp-software-defined-visibility-new-paradigm-for-it.pdf](https://wmx-api-production.s3.amazonaws.com/courses/5731/supplementary/wp-software-defined-visibility-new-paradigm-for-it.pdf)). This can provide you with a more robust ability to detect anomalies—anomalies that may suggest an incident. SDV therefore gives you a high-level perspective of network flow and endpoint/user account behavior that may not be possible with traditional appliances. SDV supports designs such as zero trust and east/west ([paloaltonetworks.com/cyberpedia/what-is-a-zero-trust-architecture](https://www.paloaltonetworks.com/cyberpedia/what-is-a-zero-trust-architecture)), plus implementation of security orchestration and automated response (SOAR).

FOG AND EDGE COMPUTING

Most of the cloud services we have considered so far are "user-facing." They support applications that human users interact with, such as video streaming, CRM, business analytics, email and conferencing, endpoint protection analytics, and so on. However, a very large and increasing amount of cloud data processing takes place with data generated by Internet of Things (IoT) devices and sensors. Industrial processes and even home automation are availability-focused. While confidentiality and integrity are still important concerns, service interruption in an operational technology network can be physically dangerous. Consequently, there is a strong requirement to retrieve and analyze IoT data with low latency.

A traditional data center architecture does not meet this requirement very well. Sensors are quite likely to have relatively low-bandwidth, higher latency WAN links to data networks. Sensors may generate huge quantities of data only a selection of which needs to be prioritized for analysis. Fog computing, developed by Cisco ([cisco.com/c/dam/en\_us/solutions/trends/iot/docs/computing-overview.pdf](https://wmx-api-production.s3.amazonaws.com/courses/5731/supplementary/computing-overview.pdf)), addresses these requirements by placing fog node processing resources close to the physical location for the IoT sensors. The sensors communicate with the fog node, using Wi-Fi, ZigBee, or 4G/5G, and the fog node prioritizes traffic, analyzes and remediates alertable conditions, and backhauls remaining data to the data center for storage and low-priority analysis.

Edge computing is a broader concept partially developed from fog computing and partially evolved in parallel to it. Fog computing is now seen as working within the concept of edge computing. Edge computing uses the following concepts:

* Edge devices are those that collect and depend upon data for their operation. For example, a thermometer in an HVAC system collects temperature data; the controller in an HVAC system activates the electromechanical components to turn the heating or air conditioning on or off in response to ambient temperature changes. The impact of latency becomes apparent when you consider edge devices such as self-driving automobiles.
* Edge gateways perform some pre-processing of data to and from edge devices to enable prioritization. They also perform the wired or wireless connectivity to transfer data to and from the storage and processing networks.
* Fog nodes can be incorporated as a data processing layer positioned close to the edge gateways, assisting the prioritization of critical data transmission.
* The cloud or data center layer provides the main storage and processing resources, plus distribution and aggregation of data between sites.

In security terms, the fog node or edge gateway layers represent high-value targets for both denial of service and data exfiltration attacks. \\
